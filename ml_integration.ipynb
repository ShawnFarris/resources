{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anders' modeling notebook: Reproducible data science with amie\n",
    "Anders now uses scikit learn to build a model that predicts the wine quality based on the chemical features that Fritz measured. This takes some experimentation: He tries different models and optimizes their parameters. Amie automatically tracks the model parameters and their performance along with the training and testing data. Even if this notebook is lost, he can come back re-create the models, and develop them further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import amieci\n",
    "import random\n",
    "import io\n",
    "import base64\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_eval(garden, model, train_x, test_x, train_y, test_y, parent=None, save=True):\n",
    "    \n",
    "    model.fit(train_x, train_y)\n",
    "    pred = model.predict(test_x)\n",
    "    \n",
    "    modelname = str(model).split('(')[0]   \n",
    "    model_params = model.get_params()\n",
    "    \n",
    "    leaf = garden.ct.new_leaf(parent=parent)\n",
    "    leaf.set_title(modelname)\n",
    "    leaf.set_description(modelname)\n",
    "    leaf.kvs.add('model_name', modelname)\n",
    "    leaf.kvs.load_from_dict(model_params)\n",
    "    leaf.kvs.add('mae', mae(test_y, pred))\n",
    "    \n",
    "    if save:\n",
    "        leaf.save()\n",
    "    \n",
    "    return leaf, model\n",
    "\n",
    "def mae(y, pred):\n",
    "    return np.mean(abs(y-pred))\n",
    "\n",
    "\n",
    "# Log in on amie.ai and go to \"www.amie.ai/#/user to get your API key\n",
    "garden = amieci.Garden(key=\"anders_key\")\n",
    "garden.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The right branch\n",
    "He sets the current leaf to where he left off with the basic data analysis, so he can continue working in this branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "garden.set_cl('ea1f4d24-e4fe-4806-8fac-4afcbf2e19eb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data again\n",
    "He now uses the same script as in the analysis notebook to load the data from Fritz and Francesca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data again\n",
    "tree_ids = ['0a5fd360-038f-4cd8-9294-069dea4fad66', 'f6dc0e23-81dd-4a87-84d8-bfb158025a79']\n",
    "tips = garden.tips()\n",
    "dataframes = []\n",
    "for tree in tree_ids:\n",
    "    for data_leaf in tips[tree]:\n",
    "        #I take the first (and only) file, because I can see that on the graph\n",
    "        try:\n",
    "            datafile = garden.leaves[data_leaf].download_data(filename='.csv')[0]\n",
    "            dataframes.append(pd.read_csv(datafile, sep=';',index_col=0))\n",
    "        except:\n",
    "            pass\n",
    "#Concatenate the data\n",
    "data = pd.concat(dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for modeling and store the data\n",
    "To make everything reproducible, he also adds the test train split and the normalization to the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler\n",
    "\n",
    "try:\n",
    "    data_leaf = garden.ct.new_leaf()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "garden.ct.cl.kvs.add(\"test_train_split\", \"0.25,0.75\")\n",
    "\n",
    "train_x = train.drop([\"quality\"], axis=1)\n",
    "test_x = test.drop([\"quality\"], axis=1)\n",
    "train_y = train[[\"quality\"]]\n",
    "test_y = test[[\"quality\"]]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "train_y = np.ravel(train_y)\n",
    "test_y = np.ravel(test_y)\n",
    "\n",
    "save_train_x = bytes(pd.DataFrame(train_x).to_csv(), 'utf-8')\n",
    "save_test_x = bytes(pd.DataFrame(test_x).to_csv(), 'utf-8')\n",
    "save_train_y = bytes(pd.DataFrame(train_y).to_csv(), 'utf-8')\n",
    "save_test_y = bytes(pd.DataFrame(test_y).to_csv(), 'utf-8')\n",
    "data_leaf.add_data('train_x.csv',save_train_x, caption='the split off training features')\n",
    "data_leaf.add_data('test_x.csv',save_test_x, caption='the split off testing features')\n",
    "data_leaf.add_data('train_y.csv',save_train_y, caption='the split off training labels')\n",
    "data_leaf.add_data('test_y',save_test_y, caption='the split off testing labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "garden.ct.cl.set_title(\"Test-train split, and normalization\")\n",
    "garden.ct.cl.set_description(\"\"\"The predicted column is quality; \n",
    "                             a scalar from [3,8]. All features normalized to [0,1]\"\"\")\n",
    "garden.ct.cl.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "He now runs his models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_leaf = garden.ct.new_leaf()\n",
    "model_leaf.set_title(\"The modeling plan\")\n",
    "model_leaf.set_description(\"\"\"Models we want to try: \\n \\\n",
    "Linear Regression \\n \\ \n",
    "Support Vector Machine Regression \\n \\ \n",
    "Random Forest Regression \\n \\\n",
    "Gradient Boosting Regression \\n \\\n",
    "K-Nearest Neighbors Regression \\n \"\"\")\n",
    "\n",
    "baseline = np.median(train_y)\n",
    "\n",
    "model_leaf.append_description(\"Average is used as the baseline\")\n",
    "model_leaf.kvs.add('baseline_mae', mae(test_y, baseline))\n",
    "model_leaf.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branching\n",
    "Since there might be further optimization steps for each model, Anders makes a new branch for each model by attaching the new leaf to the same leaf via ```parent = model_leaf``` which was defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_leaf, lr = train_and_eval(garden, \n",
    "                             LinearRegression(), \n",
    "                             train_x, test_x, train_y, test_y, \n",
    "                             parent=model_leaf)\n",
    "rf_leaf, rf = train_and_eval(garden, RandomForestRegressor(), \n",
    "                             train_x, test_x, train_y, test_y, \n",
    "                             parent=model_leaf)\n",
    "svr_leaf, svr = train_and_eval(garden, \n",
    "                               SVR(), \n",
    "                               train_x, test_x, train_y, test_y, \n",
    "                               parent=model_leaf)\n",
    "gb_leaf, gb = train_and_eval(garden, \n",
    "                             GradientBoostingRegressor(), \n",
    "                             train_x, test_x, train_y, test_y, \n",
    "                             parent=model_leaf)\n",
    "knn_leaf, knn = train_and_eval(garden, \n",
    "                               KNeighborsRegressor(), \n",
    "                               train_x, test_x, train_y, test_y, \n",
    "                               parent=model_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "Anders can quickly compare the models in the app by plotting mae over model name. He chooses to go further with 3 of them and optimizes their hyperparameters. Amie tracks all parameters in the optimization step. Since this might take a while, Anders can check through the app, from home or on his phone, how are things are. Afterwards he can analyse model parameters and results by plotting them in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = {'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'gamma' : [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(svr, grid)\n",
    "grid_search.fit(train_x, train_y)\n",
    "svr_leaf, svr = train_and_eval(garden, grid_search.best_estimator_,\n",
    "                               train_x, test_x, train_y, test_y, \n",
    "                               parent=svr_leaf, save=False)\n",
    "svr_leaf.kvs.add('grid', grid)\n",
    "svr_leaf.append_description(\"Hyperparameter optimization with gridsearch\")\n",
    "svr_leaf.set_title(\"Optimization SVR\")\n",
    "svr_leaf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 2)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 2)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(train_x, np.ravel(train_y))\n",
    "rf_leaf, rf = train_and_eval(garden, rf_random.best_estimator_,\n",
    "                             train_x, test_x, train_y, test_y, \n",
    "                             parent=rf_leaf, \n",
    "                             save=False)\n",
    "rf_leaf.kvs.add('grid', random_grid)\n",
    "rf_leaf.append_description(\"Hyperparameter optimization with a random grid\")\n",
    "rf_leaf.set_title(\"Optimization Random Forest\")\n",
    "rf_leaf.save()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = ['ls', 'lad', 'huber']\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "random_grid = {'loss': loss,\n",
    "                       'n_estimators': n_estimators,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'max_features': max_features}\n",
    "gb_random = RandomizedSearchCV(estimator=gb,\n",
    "                               param_distributions=random_grid,\n",
    "                               cv=4, n_iter=25, \n",
    "                               scoring = 'neg_mean_absolute_error',\n",
    "                               n_jobs = -1, verbose = 1, \n",
    "                               return_train_score = True,\n",
    "                               random_state=42)\n",
    "gb_random.fit(test_x, test_y)\n",
    "gb_leaf, gb = train_and_eval(garden, gb_random.best_estimator_,\n",
    "                               train_x, test_x, train_y, test_y, \n",
    "                               parent=gb_leaf,\n",
    "                               save=False)\n",
    "gb_leaf.kvs.add('grid', random_grid)\n",
    "gb_leaf.append_description(\"Hyperparameter optimization with a random grid\")\n",
    "gb_leaf.set_title(\"Optimization Gradient Boosting Regression\")\n",
    "gb_leaf.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the winner with graphiera and save the model\n",
    "\n",
    "A quick plot of mae over model_name in the app reveals the winner. Anders now saves the model to a new leaf and adds some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.fit(train_x, train_y)\n",
    "residuals = rf.predict(test_x) - test_y\n",
    "\n",
    "# Plot the residuals in a histogram\n",
    "fig_res = plt.figure()\n",
    "plt.hist(residuals, bins = 25,\n",
    "         edgecolor = 'black')\n",
    "plt.xlabel('Error'); plt.ylabel('Count')\n",
    "plt.title('Distribution of Residuals');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rf.feature_importances_).transpose()\n",
    "df = pd.DataFrame(data = rf.feature_importances_.reshape([1,11]), columns=data.columns.tolist()[0:-1], index=['importance'])\n",
    "df_array = np.array(df)[0]\n",
    "y = np.arange(1,12)\n",
    "plt.style.use('fivethirtyeight')\n",
    "fig_feat = plt.figure()\n",
    "plt.barh(y,df_array,height=0.5)\n",
    "plt.yticks(y,list(df.columns))\n",
    "plt.title('Feature Importances', fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "rf = RandomForestRegressor(random_state=60)\n",
    "from sklearn.externals import joblib\n",
    "file = io.BytesIO()\n",
    "joblib.dump(rf, file) \n",
    "file.flush()\n",
    "winning_model = base64.b64encode(file.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "garden.ct.new_leaf(parent = rf_leaf)\n",
    "garden.ct.cl.set_title(\"Winning model\")\n",
    "garden.ct.cl.set_description(\"\"\"The best model was the #model_name RandomForestRegressor#. The model is stored in a pickle $File 1$. It can be downloaded and re-run with scikit-learn.\n",
    "Residuals look fine (see $Img 2$), they are well centered around 0. With this relatively quick process and some re-measuring of the data, we could get to #mae 0.399#, which is quite OK, since the quality is given in increments of 1. \n",
    "The models improved quite a bit with optimization, plot #model_name # over #mae # to see the result. Some more work on this should get us even closer.\n",
    "More more analysis and visualization of the forest should also reveal the non-linear correlations between the features, but let's first see what we can control. \n",
    "The most important features are sulfates and alcohol content, see $Img 3$.\n",
    "\"\"\")\n",
    "garden.ct.cl.add_data('rf.pkl', winning_model, caption=\"the winning model\")\n",
    "garden.ct.cl.add_plot('residuals.png',fig_res,caption='histogram over the residuals')\n",
    "garden.ct.cl.add_plot('features.png',fig_feat,caption='histogram over the residuals')\n",
    "garden.ct.cl.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
